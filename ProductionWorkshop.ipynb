{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to become a Spacebar Artist w/ Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of people love music, but not a lot of people know how to start working with it. I come from a purely engineering background and have close to 0 understanding about music theory in general. Here, my goal is to cook up some documents that should serve as both an introduction to Python, but also an introduction to signals and machine learning applications - we will avoid math in this notebook and simply focus on applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with this, you will need a few things. Ideally you are using either VSCode or Linux, so interacting with the terminal will be straightforward.\n",
    "First, we need to ensure that you can work with JupyterNotebook from an IDE. To do this, we need to create a virtual environment. If you don't already, go into your terminal:\n",
    "```\n",
    "python3 -m venv PW\n",
    "source PW/bin/activate\n",
    "pip install librosa spleeter ipykernel matplotlib scipy numpy sounddevice\n",
    "python -m ipykernel install --user --name=PW\n",
    "sudo apt install ffmpeg\n",
    "sudo apt-get install portaudio19-dev\n",
    "```\n",
    "\n",
    "Once those are done installing, we can start cooking. To test if there are any dependencies missing, just import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 01:00:59.048104: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import spleeter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as sig\n",
    "from scipy.signal import cheby2, sosfilt, lfilter, resample\n",
    "from spleeter.audio.adapter import AudioAdapter\n",
    "from spleeter.separator import Separator\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we should also import os to conveniently work with data in folders. For your convenience, we will format a lot of this for you, but it might be insightful to learn how to use the os library eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When producing, you will ultimately need data to manipulate in order to make your songs/remixes. Gathering this data should be relatively straight forward if you are just mixing songs - you sample the song as you like and then you do magic, pretty straight forward.\n",
    "\n",
    "On the other hand, getting live samples, i.e. your voice or some instrument can be difficult. To help overcome this, engineers developed a lot of straightforward recording techniques, especially for instruments that can easily be sampled digitally (keyboards, guitars, etc). But some instruments require the old microphone to properly capture input, therefore this section will have a focus on capturing microphone input and we will ignore other signals (if you like this, maybe we can do a part 2 where we discuss how to capture microphone input in software from certain instruments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we need?\n",
    "\n",
    "We need to sample the data that the microphone is capturing, depending on what you have available, it is possible that your laptop already has a built-in (crappy) microphone, which is perfect! It is also possible that you connected an external microphone via USB to your computer, which will also work out for us. The difficult part about using a microphone is the need to actually sample it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sounddevice\n",
    "import sounddevice as sd # type: ignore\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# Set the duration of the recording in seconds\n",
    "duration = 5.0  # seconds\n",
    "\n",
    "# Set the sample rate\n",
    "sample_rate = 44100\n",
    "\n",
    "# Record audio\n",
    "recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=2)\n",
    "\n",
    "# Wait for the recording to finish\n",
    "sd.wait()\n",
    "\n",
    "# Save the recording to a file\n",
    "write('sample.wav', sample_rate, recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "\n",
    "# Set the duration of the recording in seconds\n",
    "duration = 5.0  # seconds\n",
    "\n",
    "# Set the sample rate\n",
    "sample_rate = 44100\n",
    "\n",
    "# Set the gain\n",
    "gain = 0.5\n",
    "\n",
    "# Define a callback function to process the audio\n",
    "def callback(indata, outdata, frames, time, status):\n",
    "    outdata[:] = indata * gain\n",
    "\n",
    "# Create an input-output stream\n",
    "with sd.Stream(callback=callback, samplerate=sample_rate, channels=2):\n",
    "    print(\"Press Enter to quit\")\n",
    "    input()  # sleep for the duration of the recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to work with the data in real-time like doing auto-tune for a performance (which is an overcomplication for most people), then you could do something like the following example. To break it down, we create an *audio stream object* which will stream the microphone data into the *callback* function for real-time processing. The data is appended to the indata stream in blocks of size *blocksize*, meaning a certain amount of *frames* will be written indata at every sample iteration. Finally, this data is recorded to the output list *recording*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filter(lowcut, highcut, fs, order=12):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "b,a = create_filter(20, 200, 44100) #bass filter\n",
    "\n",
    "fake_audio = np.ones(1000)\n",
    "\n",
    "y = lfilter(b, a, fake_audio)\n",
    "print(len(b))\n",
    "print(len(a))\n",
    "print(len(y))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are a few checks to make sure that your microphone can actually handle the processing you will be doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   0 Razer Kraken 7.1 Chroma: USB Audio (hw:0,0), ALSA (2 in, 0 out)\n",
       "   1 NexiGo N60 FHD Webcam: USB Audio (hw:1,0), ALSA (1 in, 0 out)\n",
       "   2 HDA Intel PCH: ALC1220 Analog (hw:2,0), ALSA (2 in, 6 out)\n",
       "   3 HDA Intel PCH: ALC1220 Digital (hw:2,1), ALSA (0 in, 2 out)\n",
       "   4 HDA Intel PCH: ALC1220 Alt Analog (hw:2,2), ALSA (2 in, 0 out)\n",
       "   5 HDA Intel PCH: HDMI 0 (hw:2,3), ALSA (0 in, 2 out)\n",
       "   6 HDA Intel PCH: HDMI 1 (hw:2,7), ALSA (0 in, 8 out)\n",
       "   7 HDA Intel PCH: HDMI 2 (hw:2,8), ALSA (0 in, 8 out)\n",
       "   8 HDA Intel PCH: HDMI 3 (hw:2,9), ALSA (0 in, 8 out)\n",
       "   9 HDA NVidia: HDMI 0 (hw:3,3), ALSA (0 in, 8 out)\n",
       "  10 HDA NVidia: HDMI 1 (hw:3,7), ALSA (0 in, 8 out)\n",
       "  11 HDA NVidia: HDMI 2 (hw:3,8), ALSA (0 in, 8 out)\n",
       "  12 HDA NVidia: HDMI 3 (hw:3,9), ALSA (0 in, 8 out)\n",
       "  13 sysdefault, ALSA (128 in, 0 out)\n",
       "  14 spdif, ALSA (2 in, 0 out)\n",
       "  15 samplerate, ALSA (128 in, 0 out)\n",
       "  16 speexrate, ALSA (128 in, 0 out)\n",
       "  17 pulse, ALSA (32 in, 32 out)\n",
       "  18 upmix, ALSA (8 in, 0 out)\n",
       "  19 vdownmix, ALSA (6 in, 0 out)\n",
       "* 20 default, ALSA (32 in, 32 out)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for audio devices\n",
    "sd.query_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Razer Kraken 7.1 Chroma: USB Audio (hw:0,0)',\n",
       " 'index': 0,\n",
       " 'hostapi': 0,\n",
       " 'max_input_channels': 2,\n",
       " 'max_output_channels': 0,\n",
       " 'default_low_input_latency': 0.008684807256235827,\n",
       " 'default_low_output_latency': -1.0,\n",
       " 'default_high_input_latency': 0.034829931972789115,\n",
       " 'default_high_output_latency': -1.0,\n",
       " 'default_samplerate': 44100.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if your input device can sample at 44100 Hz\n",
    "#check if your input device can use 1 or 2 channels\n",
    "sd.query_devices(0, 'input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press Enter to quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored from cffi callback <function _StreamBase.__init__.<locals>.callback_ptr at 0x7904b160b5b0>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jibby2k1/MusicWorkshops/ProgrammingProduction/PW/lib/python3.10/site-packages/sounddevice.py\", line 886, in callback_ptr\n",
      "    return _wrap_callback(\n",
      "  File \"/home/jibby2k1/MusicWorkshops/ProgrammingProduction/PW/lib/python3.10/site-packages/sounddevice.py\", line 2687, in _wrap_callback\n",
      "    callback(*args)\n",
      "  File \"/tmp/ipykernel_538524/1648625499.py\", line 15, in callback\n",
      "NameError: name 'b' is not defined\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28minput\u001b[39m()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Convert the list to a numpy array and save it to a file\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m recording_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecording\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# concatenate the arrays\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Write the recording array to the output file with the specified sample rate\u001b[39;00m\n\u001b[1;32m     29\u001b[0m sf\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecording.wav\u001b[39m\u001b[38;5;124m'\u001b[39m, recording_array, sample_rate)\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from scipy.signal import butter\n",
    "import soundfile as sf\n",
    "\n",
    "# Set the sample rate and the gain\n",
    "sample_rate = 44100\n",
    "gain = 0.5  # Gain factor\n",
    "\n",
    "# Create a list to store the audio data\n",
    "recording = []\n",
    "\n",
    "# Define a callback function to process the audio\n",
    "def callback(indata, outdata, frames, time, status):\n",
    "    processed = lfilter(b, a, indata)\n",
    "    # Write the processed data to the output\n",
    "    outdata = processed\n",
    "    # Append the processed data to the recording\n",
    "    recording.append(processed.copy())\n",
    "\n",
    "# Create an input-output stream\n",
    "with sd.Stream(callback=callback, samplerate=sample_rate, channels=2, blocksize=2048):\n",
    "    print(\"Press Enter to quit\")\n",
    "    input()\n",
    "\n",
    "# Convert the list to a numpy array and save it to a file\n",
    "recording_array = np.concatenate(recording)  # concatenate the arrays\n",
    "# Write the recording array to the output file with the specified sample rate\n",
    "sf.write('recording.wav', recording_array, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief explanation of the data you will receive. I manually labeled the files I am working with in a convenient way that 1) Holds Order Information and 2) Contains Relevant MetaInformation (mostly for readability and interpretability). Therefore the data might look something like: ```1_songName_author.mp3```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'Songs'  # Replace with the actual folder path\n",
    "\n",
    "file_list = []\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_list.append(file_name)\n",
    "\n",
    "file_list.sort()\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above that  a few things are done. We managed to figure out all the different song files and sort them by order to mix.\n",
    "\n",
    "The next step is to get the components of the songs that we will be using. For DJ Mixing, the most useful parts of the songs to interact with are **Vocals** and **Accompaniment**. We will use spleeter to do all of the hard machine learning work to split the song into these 2 stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_adapter = AudioAdapter.default()\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs('Songs/vocals', exist_ok=True)\n",
    "os.makedirs('Songs/accompaniment', exist_ok=True)\n",
    "\n",
    "for file_name in file_list[9:]:\n",
    "    if not file_name.endswith('.mp3'):\n",
    "        continue\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    waveform, fs = audio_adapter.load(file_path)\n",
    "    print(waveform.shape, fs)\n",
    "    \n",
    "    separator = Separator('spleeter:2stems')\n",
    "    prediction = separator.separate(waveform)\n",
    "\n",
    "    del waveform\n",
    "    gc.collect()\n",
    "\n",
    "    print(prediction)\n",
    "    print(prediction['vocals'].shape)\n",
    "    print(prediction['accompaniment'].shape)\n",
    "    \n",
    "    audio_adapter.save('Songs/vocals/' + file_name, prediction['vocals'], fs)\n",
    "    audio_adapter.save('Songs/accompaniment/' + file_name, prediction['accompaniment'], fs)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title('Vocals')\n",
    "    plt.plot(prediction['vocals'])\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title('Accompaniment')\n",
    "    plt.plot(prediction['accompaniment'])\n",
    "    plt.show()\n",
    "\n",
    "    # Delete the variables and free up the memory\n",
    "    del prediction\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! So now we have all the songs split into the different parts that most DJs address throughout their songs. The next DJ concept to tackle is \"Signal Processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mix, DJ's typically play with a few knobs and buttons on their DJ Mixer board which will allow them to either control frequency content, or apply interesting song-effects. In general, an understanding of these 2 is the bare minimum to properly DJ a set (live or pre-made), so let's dive into these.\n",
    "\n",
    "### Effects\n",
    "\n",
    "Adding effects or \"Fx\" to an audio signal is a creative and interesting way to alter your audio so that it can sound better. We will discuss a few techniques that are used to help spice up a song or transition into a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat/Loop\n",
    "\n",
    "The most simple effect that someone can do is to loop a part of a song - there are many musical implications about this but we do not need to dive into them. Consider a portion of a song from t = k $\\to$ k+T. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeSong = [i for i in range(1000)]\n",
    "k = 250\n",
    "T = 50\n",
    "reps = 3 # Number of repetitions\n",
    "newFakeSong = fakeSong[:k] + fakeSong[k:k+T] * reps + fakeSong[k+T:]\n",
    "plt.figure();\n",
    "plt.plot(newFakeSong);\n",
    "plt.plot(fakeSong);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency Control\n",
    "When mixing, the **MOST** essential effectin DJing is controlling frequency. To succesfully mix songs together, the songs must have the same BPM (or be integer scalar multiples of the other). Songs are produced for the public and many times their frequencies do not match, to fix this is actually really simple, we just need to find the BPM in a song and match it to the other song's BPM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_speed_change(song, speed_change_factor):\n",
    "    # Apply the speed change\n",
    "    y_changed = resample(song, int(len(song)*speed_change_factor))\n",
    "    # Save the modified audio to a new file\n",
    "    return y_changed\n",
    "def bpm_match(song1, song2 , sr=44100):\n",
    "    # Get the BPM of song1\n",
    "    song1_bpm = get_bpm(song1)\n",
    "    # Get the BPM of song2\n",
    "    song2_bpm = get_bpm(song2)\n",
    "    # Calculate the speed change factor\n",
    "    speed_change_factor = song2_bpm / song1_bpm\n",
    "    # Apply the speed change to song1\n",
    "    return apply_speed_change(song1, speed_change_factor, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sometimes the BPM you are using sucks and you want to change it to a better one. Let's take \"Leave Me Like This\" by Skrillex.\n",
    "#The BPM of the song is 125. Let's change it to 135.\n",
    "audio_adapter = AudioAdapter.default()\n",
    "song = 'Songs/Baddadan.mp3'\n",
    "song, _ = audio_adapter.load(song)\n",
    "new_song = apply_speed_change(song, (87.5/75))\n",
    "audio_adapter.save('Songs/Baddadan_150.mp3', new_song, 44100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find how much memory song occupies\n",
    "song = 'Songs/Tears_70.mp3'\n",
    "song, _ = audio_adapter.load(song)\n",
    "new_song = apply_speed_change(song, (70/87))\n",
    "audio_adapter.save('Songs/Tears_174.mp3', new_song, 44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equalization/Filtering\n",
    "One of the most essential effects in DJing is controlling frequency content. There are many times when a DJ may want to control how loud a part of a song is with respect to the rest of the rest of the song. I.E. if you want to emphasize a song's vocals, you may consider amplifying the treble and filtering the bass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T1 and T2 are the start and end times of the song segment to be equalized\n",
    "def apply_equalization(song, T1, T2, bass_gain=1, other_gain=1, treble_gain=1, sr=22050):\n",
    "    def cheby2_bandpass(lowcut, highcut, fs, ripple, order=5):\n",
    "        nyq = 0.5 * fs\n",
    "        low = lowcut / nyq\n",
    "        high = highcut / nyq\n",
    "        sos = cheby2(order, ripple, [low, high], btype='band', output='sos')\n",
    "        return sos\n",
    "    def cheby2_bandpass_filter(data, lowcut, highcut, fs, ripple, order=5):\n",
    "        sos = cheby2_bandpass(lowcut, highcut, fs, ripple, order=order)\n",
    "        y = sosfilt(sos, data)\n",
    "        return y\n",
    "    N1, N2 = librosa.time_to_samples([T1, T2], sr=fs)\n",
    "    lowbass, highbass, lowtreble, hightreble = 20, 300, 2000, 20000\n",
    "    bass = cheby2_bandpass_filter(song[N1:N2], lowbass, highbass, fs, ripple=20, order=6)\n",
    "    other = cheby2_bandpass_filter(song[N1:N2], highbass, lowtreble, fs, ripple=20, order=6)\n",
    "    treble = cheby2_bandpass_filter(song[N1:N2], lowtreble, hightreble, fs, ripple=20, order=6)\n",
    "    equalized_song = song.copy()\n",
    "    equalized_song[N1:N2] = bass_gain*bass + other_gain*other + treble_gain*treble\n",
    "    return equalized_song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reverb\n",
    "\n",
    "A great effect to use on any song, it simulates the effect of the reflections of sounds in a physical space. It can make audio sound like you are inside of a barrel or a concert hall, depending on the desired outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_reverb(song, reverb_time, decay):\n",
    "    # Generate reverb impulse response\n",
    "    t = np.arange(0, reverb_time, 1/(44100))\n",
    "    reverb = np.exp(-decay * t)\n",
    "    # Apply reverb to the signal\n",
    "    reverb_signal = song + np.convolve(song, reverb, mode='same')\n",
    "    reverb_signal = reverb_signal / np.max(np.abs(reverb_signal))\n",
    "    return reverb_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delay/Echoes\n",
    "\n",
    "Another effect which is interesting to listen to, just as the name implies, it echoes the past song samples which can give an interesting effect similar to that of filtering. It can cluster sounds, and ultimately add some \"chaos\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_echoes(signal, delay, decay):\n",
    "    # Calculate the number of samples to delay\n",
    "    delay_samples = int(delay * 44100)\n",
    "    # Create an empty array to store the echoed signal\n",
    "    echoed_signal = np.zeros_like(signal)\n",
    "    # Apply the echo effect\n",
    "    for i in range(delay_samples, len(signal)):\n",
    "        echoed_signal[i] = signal[i] + decay * signal[i - delay_samples]\n",
    "    return echoed_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transient(signal, attack_time_percent, fs=44100):\n",
    "    # Calculate the number of samples for attack and decay\n",
    "    attack_samples = int(attack_time_percent * len(signal))\n",
    "    decay_samples = len(signal) - attack_samples\n",
    "    # Create the transient envelope\n",
    "    envelope = np.zeros_like(signal)\n",
    "    envelope[:attack_samples] = np.linspace(0, 1, attack_samples)\n",
    "    envelope[attack_samples:] = np.linspace(1, 0, decay_samples)\n",
    "    # Apply the transient effect to the signal\n",
    "    transient_signal = lfilter(envelope, [1], signal)\n",
    "    return transient_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixing\n",
    "\n",
    "Mixing itself can be pretty difficult, it is mostly done by ear but there are some rules that all EDM tend to follow. I'm not sure if there's an explicit name for it but I'd like to call it the $2^n$ beat rule, what this says is that all events in a song will happen on iterations of $2^n$, i.e. every 4-16 beats something new will happen in a song. To learn this I highly suggest looking at beat markers on a song in mixing software like Serato DJ Lite or Rekordbox (or whatever you use). What I'd like to implement is a function that mixes 2 (or more) audio signals in a weighed manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audo_mix(songs, weights, max_amplitude=1):\n",
    "    # Normalize the weights\n",
    "    weights = weights / np.sum(weights)\n",
    "    # Mix the songs\n",
    "    mixed_song = np.zeros_like(songs[0])\n",
    "    for i in range(len(songs)):\n",
    "        mixed_song += weights[i] * songs[i]\n",
    "    # Normalize the amplitude\n",
    "    mixed_song = max_amplitude * mixed_song / np.max(np.abs(mixed_song))\n",
    "    return mixed_song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these tools you've learned how to:\n",
    "\n",
    "1) Decompose Songs into Stems (for sampling)\n",
    "\n",
    "2) Apply Effects (Fx)\n",
    "\n",
    "and\n",
    "\n",
    "3) Mix Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
